{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "pacman_ddqn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4203506e",
        "outputId": "84b0375f-2f29-4c25-f551-b282bc298461"
      },
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    %tensorflow_version 2.x\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "if COLAB:\n",
        "  !sudo apt-get install -y xvfb ffmpeg\n",
        "  !pip install -q 'gym==0.10.11'\n",
        "  !pip install -q 'imageio==2.4.0'\n",
        "  !pip install -q PILLOW\n",
        "  !pip install -q 'pyglet==1.3.2'\n",
        "  !pip install -q pyvirtualdisplay\n",
        "  !pip install -q --upgrade tensorflow-probability\n",
        "  !pip install -q tf-agents\n",
        "\n",
        "  # ROM dependencies for atari games\n",
        "  ! wget http://www.atarimania.com/roms/Roms.rar\n",
        "  ! mkdir /content/ROM/\n",
        "  ! unrar e /content/Roms.rar /content/ROM/\n",
        "  ! python -m atari_py.import_roms /content/ROM/\n",
        "\n",
        "  # For visualisation on colab\n",
        "  !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "  !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "  # gym installation\n",
        "  !apt-get update > /dev/null 2>&1\n",
        "  !apt-get install cmake > /dev/null 2>&1\n",
        "  !pip install --upgrade setuptools 2>&1\n",
        "  !pip install ez_setup > /dev/null 2>&1\n",
        "  !pip install gym[atari] > /dev/null 2>&1\n",
        "\n",
        "  # For use of GPU\n",
        "  %tensorflow_version 2.x\n",
        "  import tensorflow as tf\n",
        "  device_name = tf.test.gpu_device_name()\n",
        "  if device_name != '/device:GPU:0':\n",
        "    raise SystemError('GPU device not found')\n",
        "    global CPU\n",
        "    CPU = True\n",
        "  print('Found GPU at: {}'.format(device_name))\n",
        "\n",
        "  # Required to save models in HDF5 format\n",
        "  !pip install pyyaml h5py"
      ],
      "id": "4203506e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: using Google CoLab\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:3.4.8-0ubuntu0.2).\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.9).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 59 not upgraded.\n",
            "\u001b[31mERROR: tf-agents 0.8.0 has requirement gym>=0.17.0, but you'll have gym 0.10.11 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tf-agents 0.8.0 has requirement gym>=0.17.0, but you'll have gym 0.10.11 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tf-agents 0.8.0 has requirement tensorflow-probability==0.12.2, but you'll have tensorflow-probability 0.13.0 which is incompatible.\u001b[0m\n",
            "--2021-07-15 11:01:01--  http://www.atarimania.com/roms/Roms.rar\n",
            "Resolving www.atarimania.com (www.atarimania.com)... 195.154.81.199\n",
            "Connecting to www.atarimania.com (www.atarimania.com)|195.154.81.199|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11128004 (11M) [application/x-rar-compressed]\n",
            "Saving to: ‘Roms.rar.2’\n",
            "\n",
            "Roms.rar.2          100%[===================>]  10.61M   821KB/s    in 14s     \n",
            "\n",
            "2021-07-15 11:01:15 (792 KB/s) - ‘Roms.rar.2’ saved [11128004/11128004]\n",
            "\n",
            "mkdir: cannot create directory ‘/content/ROM/’: File exists\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from /content/Roms.rar\n",
            "\n",
            "\n",
            "Would you like to replace the existing file /content/ROM/HC ROMS.zip\n",
            "11826711 bytes, modified on 2019-12-22 11:24\n",
            "with a new one\n",
            "11826711 bytes, modified on 2019-12-22 11:24\n",
            "\n",
            "[Y]es, [N]o, [A]ll, n[E]ver, [R]ename, [Q]uit "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ioqp7yRmEEpU"
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "from google.colab import files,drive\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment \n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  files.download(mp4list[0])\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    print(video)\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "id": "Ioqp7yRmEEpU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "id": "6cnPPyY_EG6x",
        "outputId": "6830a54a-9cc5-442e-918c-b127bd5ee87c"
      },
      "source": [
        "# env = wrap_env(gym.make(\"MountainCar-v0\"))\n",
        "# with tf.device('/device:GPU:0'):   \n",
        "\n",
        "for i in range(3):\n",
        "  env = wrap_env(gym.make(\"MsPacman-v0\")) \n",
        "  observation = env.reset()\n",
        "  while True:\n",
        "        \n",
        "        # env.render()\n",
        "        #your agent goes here\n",
        "        action = env.action_space.sample() \n",
        "        if(i==1): action = env.action_space.sample()\n",
        "\n",
        "        observation, reward, done, info = env.step(action)    \n",
        "            \n",
        "        if done:\n",
        "            env.close()\n",
        "            show_video()\n",
        "            break;\n",
        "\n",
        "# show_video()"
      ],
      "id": "6cnPPyY_EG6x",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-96-8aa0dda94e06>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTD9EeUYrfBt"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "id": "iTD9EeUYrfBt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51356073"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "import scipy.misc\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "from skimage import transform\n",
        "from google.colab import files,drive\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.math import add,reduce_mean\n",
        "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras.models import Model, load_model, Sequential\n",
        "from tensorflow.keras.losses import MeanSquaredError \n",
        "from tensorflow.keras.metrics import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.initializers import random_uniform, glorot_uniform, constant, identity\n"
      ],
      "id": "51356073",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a4203a0"
      },
      "source": [
        "env = wrap_env(gym.make('MsPacman-v0'))\n",
        "observation = env.reset()\n",
        "input_shape = preprocess_observation(observation).shape\n",
        "# input_shape = observation.shape\n",
        "classes = env.action_space.n\n",
        "env.close()\n",
        "# print(input_shape, classes)"
      ],
      "id": "7a4203a0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0df84790"
      },
      "source": [
        "def identity_block(X, f, filters, training=True, initializer=random_uniform):\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value. You'll need this later to add back to the main path. \n",
        "    X_shortcut = X\n",
        "    \n",
        "    # First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = 1, strides = (1,1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training = training) # Default axis\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    ## Second component of main path (≈3 lines)\n",
        "    X = Conv2D(filters = F2, kernel_size = f, strides = (1,1), padding = 'same', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training = training) # Default axis\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    ## Third component of main path (≈2 lines)\n",
        "    X = Conv2D(filters = F3, kernel_size = 1, strides = (1,1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training = training) # Default axis\n",
        "    \n",
        "    ## Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
        "    X = Add()([X_shortcut,X])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X"
      ],
      "id": "0df84790",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cbd5f7b"
      },
      "source": [
        "def convolutional_block(X, f, filters, s = 2, training=True, initializer=glorot_uniform):\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "    ##### MAIN PATH #####\n",
        "    \n",
        "    # First component of main path glorot_uniform(seed=0)\n",
        "    X = Conv2D(filters = F1, kernel_size = 1, strides = (s, s), padding='valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training=training)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    ## Second component of main path (≈3 lines)\n",
        "    X = Conv2D(filters = F2, kernel_size = f, strides = (1,1), padding = 'same', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training = training) # Default axis\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    ## Third component of main path (≈2 lines)\n",
        "    X = Conv2D(filters = F3, kernel_size = 1, strides = (1,1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training = training) # Default axis\n",
        "    \n",
        "    ##### SHORTCUT PATH ##### (≈2 lines)\n",
        "    X_shortcut = Conv2D(filters = F3, kernel_size = 1, strides = (s,s), padding = 'valid', kernel_initializer = initializer(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis = 3)(X_shortcut, training = training) # Default axis\n",
        "    \n",
        "    # Final step: \n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X"
      ],
      "id": "9cbd5f7b",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7fec656"
      },
      "source": [
        "def ResNet50(input_shape, classes): # input and classes depends on gym environment\n",
        "    \"\"\"\n",
        "    Stage-wise implementation of the architecture of the popular ResNet50:\n",
        "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
        "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> FLATTEN -> DENSE \n",
        "\n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "    classes -- integer, number of classes\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input = Input(input_shape)\n",
        "\n",
        "    \n",
        "    # Zero-Padding\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "    \n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], s = 1)\n",
        "    X = identity_block(X, 3, [64, 64, 256])\n",
        "    X = identity_block(X, 3, [64, 64, 256])\n",
        "    \n",
        "    ## Stage 3 (≈4 lines)\n",
        "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], s = 2)\n",
        "    X = identity_block(X, 3, [128, 128, 512])\n",
        "    X = identity_block(X, 3, [128, 128, 512])\n",
        "    X = identity_block(X, 3, [128, 128, 512]) \n",
        "    \n",
        "    ## Stage 4 (≈6 lines)\n",
        "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], s = 2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "\n",
        "    ## Stage 5 (≈3 lines)\n",
        "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], s = 2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048])\n",
        "    X = identity_block(X, 3, [512, 512, 2048])\n",
        "\n",
        "    ## AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
        "    X = AveragePooling2D(pool_size = (2,2))(X)\n",
        "\n",
        "    # output layer\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='softmax', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    \n",
        "    # Create model\n",
        "    model = Model(inputs = X_input, outputs = X)\n",
        "\n",
        "    return model"
      ],
      "id": "a7fec656",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b379db3a"
      },
      "source": [
        "def Deepminds_model(input_shape, classes):\n",
        "        \n",
        "        \n",
        "    X_input = Input(input_shape)\n",
        "    \n",
        "    X = Conv2D(32, (8, 8), strides = (4, 4), kernel_initializer = glorot_uniform(seed=0))(X_input)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2D(64, (4, 4), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2D(64, (3, 3), strides = (1, 1), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(512, activation='relu', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = Dense(classes, activation= None, kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    model = Model(inputs = X_input, outputs = X)\n",
        "\n",
        "    return model\n",
        "    \n",
        "    "
      ],
      "id": "b379db3a",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgRyLhM8Mrz7"
      },
      "source": [
        "def ConvDuelingDQN(input_shape, classes):\n",
        "\n",
        "    \"\"\"\n",
        "    We split the network into two separate streams, one for estimating the state-value and\n",
        "    the other for estimating state-dependent action advantages.\n",
        "\n",
        "    The last module of the neural network implements forward mapping shown below: \n",
        "\n",
        "    Q(s,a;theta,alpha.beta) = V(s;theta,beta)  + A(s,a;theta,alpha) - sigma A(s,a;theta,alpha)/|A|\n",
        "    \"\"\"\n",
        "\n",
        "    X_input = Input(input_shape)\n",
        "    \n",
        "    X = Conv2D(32, (8, 8), strides = (4, 4), kernel_initializer = glorot_uniform(seed=0))(X_input)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2D(64, (4, 4), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2D(64, (3, 3), strides = (1, 1), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Flatten()(X)\n",
        "\n",
        "    # Value stream\n",
        "    V = Dense(128,activation='relu', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    V = Dense(1,activation=None, kernel_initializer = glorot_uniform(seed=0))(V)\n",
        "\n",
        "    # Action stream\n",
        "    A = Dense(128,activation='relu',kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    A = Dense(classes, activation=None, kernel_initializer= glorot_uniform(seed=0))(A)\n",
        "\n",
        "    Q = add(V, add(A, -reduce_mean(A)/ classes ))\n",
        "\n",
        "    model = Model(inputs = X_input, outputs = Q)\n",
        "\n",
        "    return model\n"
      ],
      "id": "BgRyLhM8Mrz7",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54dd6a46"
      },
      "source": [
        "def scale_lumininance(img):\n",
        "    return np.dot(img[...,:3],[0.299,0.587,0.114])"
      ],
      "id": "54dd6a46",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b86ff3b"
      },
      "source": [
        "def preprocess_observation(obs):\n",
        "    \"\"\"\n",
        "    To reduce the computation load we preprocess the date obtained from the atari-game env by\n",
        "    reducing the resolution and adjusting the color.\n",
        "    \n",
        "    DeepMind took the maximum pixel value over subsequent frames to reduce flickering\n",
        "    caused by the limitations of the Atari platform and then scale it from its current \n",
        "    210×160×3 resolution to 84×84.\n",
        "    \n",
        "    To convert this, we will take the luminance channel (denoted as Y) from the image, which is the our RGB channel, and apply linear weights to \n",
        "    the channel to transform it according to the relative luminance.\n",
        "    Y = 0.299R + 0.587G + 0.114B\n",
        "    \"\"\"\n",
        "    \n",
        "    obs_gray = scale_lumininance(obs)\n",
        "    obs_trans = transform.resize(obs_gray,(84,84))\n",
        "    return np.moveaxis(obs_trans, 1, 0)"
      ],
      "id": "6b86ff3b",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL9CbWoK5cTM"
      },
      "source": [
        "class Prioritized_replay_buffer():\n",
        "    def __inti__(self,buffer_size):\n",
        "      self.buffer = deque(maxlen = buffer_size)\n",
        "      self.buffer_size = buffer_size\n",
        "      self.priorities = deque(maxlen = buffer_size) # a different deque\n",
        "      self.initialize(initial_collect_step)\n",
        "      \n",
        "    def initialize(self,initial_collect_step):\n",
        "        observation = env.reset()\n",
        "        for i in range(initial_collect_steps):\n",
        "            current_state = preprocess_observation(observation)\n",
        "            # current_state = observation\n",
        "            action = env.action_space.sample()\n",
        "            observation, reward, done , _ = env.step(action)\n",
        "            next_state = preprocess_observation(observation)\n",
        "            # next_state = observation\n",
        "            if done: observation = env.reset()\n",
        "        \n",
        "            experience = (current_state,action,reward,done,next_state)\n",
        "            self.add(experience)\n",
        "        \n",
        "        env.close()\n",
        "    \n",
        "    def add(self,experience):\n",
        "      self.buffer.append(experience)\n",
        "      self.priorities.append(max(self.priorities, default = 1))\n",
        "\n",
        "    def get_probabilities(self, priority_scale):\n",
        "      scaled_priorities = np.array(self.priorities) ** priority_scale\n",
        "      sample_probabilities = scaled_priorities/ sum(scaled_priorities)\n",
        "\n",
        "    def get_importance(self, probabilities):\n",
        "      importance = 1/self.buffer_size * 1/probabilities\n",
        "      # this has been normalized to keep the update_step size bounded\n",
        "      importance_normalized = importance/max(importance)\n",
        "      return importance_normalized\n",
        "\n",
        "    def sample(self, batch_size, priority_scale = 1.0):\n",
        "      '''\n",
        "      Before application of Prioritized experience replay samples were choosen randomly using random.sample\n",
        "      But now every sample were choosen according the their priority and the weights of the network were also updated \n",
        "      in accordance to these priorities.\n",
        "\n",
        "      There can be two types of prioritization.\n",
        "      Here we are using proportional based with hyperparameter a = 0.7, b = 0.5 \n",
        "\n",
        "      # sampling                              # parameter updates\n",
        "      error = Q(s,a) - Q_target               loss = error^2\n",
        "      p = |error| + offset                    theta -> theta - alpha * w * grad\n",
        "      p_r(i) = p_i^a/(sigma p^a)              w_i = (1/N * 1/Pr_(i))^b  # importance_weight\n",
        "      \n",
        "      a => priority_scale (0.7)\n",
        "      b => 0.5       # fully compensates for the non-uniform probabilities\n",
        "      '''\n",
        "      # samples = random.sample(self.buffer,batch_size)\n",
        "      sample_size = min(len(self.buffer), batch_size)\n",
        "      sample_probs = self.get_probabilities(priority_scale)\n",
        "      sample_indices = random.choices(range(self.buffer_size), k=sample_size, weights=sample_probs)\n",
        "      samples = np.array(self.buffer)[sample_indices]\n",
        "      importance = self.get_importance(sample_probs[sample_indices])\n",
        "      return samples, importance , sample_indices\n",
        "\n",
        "    def set_priorities(self, indices, errors, offset=0.1):\n",
        "        for i,e in zip(indices, errors):\n",
        "            self.priorities[i] = abs(e) + offset\n"
      ],
      "id": "bL9CbWoK5cTM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1b15339"
      },
      "source": [
        "def initialization(input_shape , classes, initial_collect_steps, buffer_size):\n",
        "    \"\"\"\n",
        "    The target_network and prediction_network are initialized as ResNet50/DeepMind's CNN architecture with trainability\n",
        "    of prediction_network set to False.\n",
        "    \n",
        "    The Experience Reply is initialized as a deque whose each element \n",
        "    is a tuple (current state, action, reward , done, next state)\n",
        "    Few actions are executed with the environment to bootstrap the replay data.\n",
        "    \"\"\"\n",
        "    global target_network,prediction_network\n",
        "#     target_network = ResNet50(input_shape,classes)\n",
        "    target_network = Deepminds_model(input_shape,classes)  \n",
        "    # print(target_network.summary())\n",
        "    \n",
        "#     prediction_network = ResNet50(input_shape,classes)\n",
        "    prediction_network = Deepminds_model(input_shape,classes)\n",
        "    # print(prediction_network.summary())\n",
        "    \n",
        "    experience_replay = Prioritized_replay_buffer(buffer_size,initial_collect_step)"
      ],
      "id": "a1b15339",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1da5cd81"
      },
      "source": [
        "def plot(CR , episode, save_fig = True):\n",
        "    plt.plot([i+1 for i in range(episode)],CR, linewidth=4, label = \"DDQN\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"episode_reward\")\n",
        "    leg = plt.legend(loc='upper left', shadow=True)\n",
        "    plt.grid()\n",
        "    ax = plt.gca()\n",
        "    plt.xlim([0, episode])\n",
        "    \n",
        "    output_dir = os.path.join(os.getcwd(), \"output\")\n",
        "    if not os.path.exists(output_dir): os.mkdir(output_dir)\n",
        "    \n",
        "    if save_fig:\n",
        "        plt.savefig(os.path.join(output_dir, \"plot-{}th-episode.png\".format(episode)), bbox_inches=\"tight\")\n",
        "        files.download(os.path.join(output_dir, \"plot-{}th-episode.png\".format(episode)))\n",
        "    else:\n",
        "        plt.show()\n",
        "    plt.close()"
      ],
      "id": "1da5cd81",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d490d86"
      },
      "source": [
        "def eps_greedy_action(Q,eps):\n",
        "    if np.random.random() < eps: # explore\n",
        "        return np.random.randint(env.action_space.n)\n",
        "    else: # exploit\n",
        "        return np.random.choice(np.flatnonzero(Q == Q.numpy().max()))"
      ],
      "id": "2d490d86",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVVRnBdg8wn7"
      },
      "source": [
        "# @tf.function\n",
        "# def train_step(state_minibatch,target,actions):\n",
        "\n",
        "    "
      ],
      "id": "bVVRnBdg8wn7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "771b5d7a"
      },
      "source": [
        "def experiment(N_episodes,gamma,learning_rate, c, batch_size,eps, log_interval, priority_scale = 0.7):\n",
        "    \n",
        "  # Instantiate an optimizer to train the model.\n",
        "  optimizer = tf.keras.optimizers.RMSprop(learning_rate) # used by deepminds\n",
        "  # Instantiate a loss function.\n",
        "  loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "  # Prepare the metrics.\n",
        "  train_acc_metric = tf.keras.metrics.MeanSquaredError()\n",
        "  \n",
        "  CR = []\n",
        "  with tf.device('/device:GPU:0'):\n",
        "\n",
        "    for episode in range(N_episodes):\n",
        "        start_time = time.time()\n",
        "        time_step = 0\n",
        "        env = wrap_env(gym.make('MsPacman-v0'))\n",
        "        observation = env.reset()\n",
        "        rewards = [] # compared to total points earned in the game\n",
        "        while True:\n",
        "            \n",
        "            with tf.GradientTape() as tape:\n",
        "                \n",
        "                # current_state = observation\n",
        "                current_state = preprocess_observation(observation) \n",
        "                Q = prediction_network(np.array([current_state]), training=False)\n",
        "                action = eps_greedy_action(Q,eps) \n",
        "                new_observation , reward , done , _ = env.step(action)\n",
        "                rewards.append(reward)\n",
        "                # next_state = new_observation\n",
        "                next_state = preprocess_observation(new_observation)\n",
        "                \n",
        "                experience = (current_state, action, reward, done, next_state)\n",
        "                experience_replay.add(experience)\n",
        "                \n",
        "                minibatch, importance , indices = experience_replay.sample(batch_size, priority_scale)\n",
        "                state_minibatch = np.array([current_state for (current_state, action, reward, done, next_state) in minibatch])\n",
        "                actions = np.array([action for (current_state, action, reward, done, next_state) in minibatch])\n",
        "                            \n",
        "                target = []\n",
        "                for (current_state, action, reward, done, next_state) in minibatch:\n",
        "                    if done:\n",
        "                        target.append(reward)\n",
        "                    else:\n",
        "                        Q_target = target_network(np.array([next_state]),training = False)\n",
        "                        target.append(reward + gamma * Q_target.numpy().max())\n",
        "\n",
        "                prediction_network_output = prediction_network(state_minibatch, training = True)\n",
        "                prediction = [Q[action] for Q,action in zip(prediction_network_output,actions)]\n",
        "                \n",
        "                errors = abs(target - prediction)\n",
        "                experience_replay.set_priorities(indices, errors)\n",
        "                loss_value = loss_fn(target , prediction)\n",
        "                grads = tape.gradient(loss_value, prediction_network.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(grads, prediction_network.trainable_variables))\n",
        "\n",
        "                # Update training metric.\n",
        "                train_acc_metric.update_state(target, prediction)\n",
        "\n",
        "                # S -> S'\n",
        "                observation = new_observation\n",
        "                time_step+=1\n",
        "                if time_step%100 == 0 :\n",
        "                  print(\"Time_step:{}\".format(time_step))\n",
        "                \n",
        "                if episode%c ==0: \n",
        "                    target_network.set_weights(prediction_network.get_weights()) \n",
        "              \n",
        "                if done:\n",
        "                \n",
        "                    # Display metrics at the end of each episode.\n",
        "                    train_acc = train_acc_metric.result()\n",
        "                    print(\"Training acc over episode: %.4f\" % (float(train_acc),))\n",
        "\n",
        "                    # Reset training metrics at the end of each episode\n",
        "                    train_acc_metric.reset_states()\n",
        "                    \n",
        "                    CR.append(np.squeeze(np.sum(np.array(rewards))))\n",
        "\n",
        "                    env.close()\n",
        "                    if (episode%log_interval==0):\n",
        "                      show_video()\n",
        "                      output_dir = os.path.join(os.getcwd(), \"output\")\n",
        "                      if not os.path.exists(output_dir): os.mkdir(output_dir)\n",
        "                      prediction_network.save(os.path.join(output_dir,\"model-{}.h5\".format(episode/log_interval)))\n",
        "                      files.download(os.path.join(output_dir,\"model-{}.h5\".format(episode/log_interval)))\n",
        "                      plot(CR,episode+1)\n",
        "                    print(\"Episode {} finished after {} timesteps\".format(episode+1, time_step+1))\n",
        "                    print(\"{}s taken to finish this episode\".format(time.time() - start_time))\n",
        "                    break"
      ],
      "id": "771b5d7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "fe8fd3b3",
        "outputId": "7befa86d-7560-4179-c714-1c387f8ec66e"
      },
      "source": [
        "learning_rate = 2.5e-3\n",
        "\n",
        "log_interval = 100\n",
        "\n",
        "initial_collect_steps = 200\n",
        "replay_buffer_max_length = 50000\n",
        "\n",
        "gamma = 0.99\n",
        "c = 10 # not sure\n",
        "eps = 0.1\n",
        "batch_size = 32\n",
        "N_episodes = 12000\n",
        "\n",
        "initialization(input_shape,classes,initial_collect_steps,replay_buffer_max_length)\n",
        "experiment(N_episodes, gamma, learning_rate, c, batch_size,eps,log_interval)\n"
      ],
      "id": "fe8fd3b3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time_step:1\n",
            "Time_step:2\n",
            "Time_step:3\n",
            "Time_step:4\n",
            "Time_step:5\n",
            "Time_step:6\n",
            "Time_step:7\n",
            "Time_step:8\n",
            "Time_step:9\n",
            "Time_step:10\n",
            "Time_step:11\n",
            "Time_step:12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-1eb7847b570f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0minitialization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minitial_collect_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreplay_buffer_max_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-64-845c39b03a53>\u001b[0m in \u001b[0;36mexperiment\u001b[0;34m(N_episodes, gamma, learning_rate, c, batch_size, eps, log_interval)\u001b[0m\n\u001b[1;32m     41\u001b[0m                         \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                         \u001b[0mQ_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m                         \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mQ_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1028\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1029\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1030\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \"\"\"\n\u001b[1;32m    420\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 421\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    976\u001b[0m     if any(isinstance(x, (\n\u001b[1;32m    977\u001b[0m         np_arrays.ndarray, np.ndarray, float, int)) for x in input_list):\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_convert_numpy_or_python_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    866\u001b[0m   return pack_sequence_as(\n\u001b[1;32m    867\u001b[0m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m       expand_composites=expand_composites)\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mpack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites)\u001b[0m\n\u001b[1;32m    753\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msortable\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m   \"\"\"\n\u001b[0;32m--> 755\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_pack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand_composites\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m_pack_sequence_as\u001b[0;34m(structure, flat_sequence, expand_composites, sequence_fn)\u001b[0m\n\u001b[1;32m    617\u001b[0m         .format(truncate(flat_sequence, 100), type(flat_sequence)))\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_sequence\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m       raise ValueError(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIS7sPOXSFio"
      },
      "source": [
        ""
      ],
      "id": "zIS7sPOXSFio",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQRbjz6MSILs"
      },
      "source": [
        ""
      ],
      "id": "XQRbjz6MSILs",
      "execution_count": null,
      "outputs": []
    }
  ]
}