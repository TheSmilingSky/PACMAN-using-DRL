{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import tensorflow as tf\r\n",
    "import numpy as np\r\n",
    "import tensorflow.keras as keras\r\n",
    "import gym\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "env=gym.make('MsPacman-v0')\r\n",
    "print(env.observation_space)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Box(0, 255, (210, 160, 3), uint8)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "n_height = 210\r\n",
    "n_width = 160\r\n",
    "n_depth = 3\r\n",
    "n_shape = [n_height,n_width,n_depth]\r\n",
    "n_inputs = n_height * n_width * n_depth\r\n",
    "env.frameskip = 4\r\n",
    "# frameskip of 4 speeds up the process"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from collections import deque \r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense, Flatten\r\n",
    "\r\n",
    "# build the Q-Network\r\n",
    "model = Sequential()\r\n",
    "model.add(Flatten(input_shape = n_shape))\r\n",
    "model.add(Dense(512, activation='relu',name='hidden1'))\r\n",
    "model.add(Dense(9, activation='softmax', name='output'))\r\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam')\r\n",
    "model.summary()\r\n",
    "q_nn =model"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 100800)            0         \n",
      "_________________________________________________________________\n",
      "hidden1 (Dense)              (None, 512)               51610112  \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 9)                 4617      \n",
      "=================================================================\n",
      "Total params: 51,614,729\n",
      "Trainable params: 51,614,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def policy_q_nn(obs, env):\r\n",
    "    \r\n",
    "    if np.random.random() < explore_rate:\r\n",
    "        action = env.action_space.sample()\r\n",
    "    \r\n",
    "    else:\r\n",
    "        action = np.argmax(q_nn.predict(np.array([obs])))\r\n",
    "    return action\r\n",
    "# eps-greedy\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "def episode(env, policy, r_max=0, t_max=0):\r\n",
    "    \r\n",
    "    #restart\r\n",
    "    obs = env.reset()\r\n",
    "    state_prev = obs\r\n",
    "    episode_reward = 0\r\n",
    "    done = False\r\n",
    "    t = 0\r\n",
    "    env.step(0)\r\n",
    "    \r\n",
    "    #tf-fitting\r\n",
    "    while not done:\r\n",
    "        \r\n",
    "        action = policy(state_prev, env)\r\n",
    "        obs, reward, done, info = env.step(action)\r\n",
    "        \r\n",
    "        state_next = obs\r\n",
    "\r\n",
    "        memory.append([state_prev,action,reward,state_next,done])\r\n",
    "\r\n",
    "        states = np.array([x[0] for x in memory])\r\n",
    "        states_next = np.array([np.zeros(n_shape) if x[4] else x[3] for x in memory])        \r\n",
    "        q_values = q_nn.predict(states)\r\n",
    "        q_values_next = q_nn.predict(states_next)\r\n",
    "        \r\n",
    "        for i in range(len(memory)):\r\n",
    "            state_prev,action,reward,state_next,done = memory[i]\r\n",
    "            if done:\r\n",
    "                \r\n",
    "                q_values[i,action] = reward\r\n",
    "            else:\r\n",
    "                \r\n",
    "                best_q = np.amax(q_values_next[i])\r\n",
    "                bellman_q = reward + discount_rate * best_q\r\n",
    "                q_values[i,action] = bellman_q\r\n",
    "                \r\n",
    "        q_nn.fit(states,q_values,epochs=1,batch_size=50,verbose=0)\r\n",
    "        state_prev = state_next\r\n",
    "        \r\n",
    "\r\n",
    "        episode_reward += reward\r\n",
    "        if r_max > 0 and episode_reward > r_max:\r\n",
    "            break\r\n",
    "        t+=1\r\n",
    "        if t_max > 0 and t == t_max:\r\n",
    "            break\r\n",
    "    return episode_reward\r\n",
    "\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "\r\n",
    "def experiment(env, policy, n_episodes,r_max=0, t_max=0):\r\n",
    "    \r\n",
    "    rewards=np.empty(shape=[n_episodes])\r\n",
    "    for i in range(n_episodes):\r\n",
    "        val = episode(env, policy, r_max, t_max)\r\n",
    "        rewards[i]=val\r\n",
    "            \r\n",
    "    print('Policy:{}, Min reward:{}, Max reward:{}, Average reward:{}'\r\n",
    "        .format(policy.__name__,\r\n",
    "              np.min(rewards),\r\n",
    "              np.max(rewards),\r\n",
    "              np.mean(rewards)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "\r\n",
    "discount_rate = 0.9\r\n",
    "explore_rate = 0.2\r\n",
    "n_episodes = 50\r\n",
    "\r\n",
    "\r\n",
    "memory = deque(maxlen=1000)\r\n",
    "\r\n",
    "experiment(env, policy_q_nn, n_episodes)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "env.close()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}