{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "pacman_NoisyNet_n-step_PDD-DQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4203506e"
      },
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    %tensorflow_version 2.x\n",
        "    global COLAB\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "if COLAB:\n",
        "  !sudo apt-get install -y xvfb ffmpeg\n",
        "  !pip install -q 'gym==0.10.11'\n",
        "  !pip install -q 'imageio==2.4.0'\n",
        "  !pip install -q PILLOW\n",
        "  !pip install -q 'pyglet==1.3.2'\n",
        "  !pip install -q pyvirtualdisplay\n",
        "  !pip install -q --upgrade tensorflow-probability\n",
        "  !pip install -q tf-agents\n",
        "\n",
        "  # ROM dependencies for atari games\n",
        "  ! wget http://www.atarimania.com/roms/Roms.rar\n",
        "  ! mkdir /content/ROM/\n",
        "  ! unrar e /content/Roms.rar /content/ROM/\n",
        "  ! python -m atari_py.import_roms /content/ROM/\n",
        "\n",
        "  # For visualisation on colab\n",
        "  !pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "  !apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "\n",
        "  # gym installation\n",
        "  !apt-get update > /dev/null 2>&1\n",
        "  !apt-get install cmake > /dev/null 2>&1\n",
        "  !pip install --upgrade setuptools 2>&1\n",
        "  !pip install ez_setup > /dev/null 2>&1\n",
        "  !pip install gym[atari] > /dev/null 2>&1\n",
        "\n",
        "  # Required to save models in HDF5 format\n",
        "  !pip install pyyaml h5py\n",
        "  \n",
        "  # For use of GPU\n",
        "  %tensorflow_version 2.x\n",
        "  import tensorflow as tf\n",
        "  device_name = tf.test.gpu_device_name()\n",
        "  if device_name != '/device:GPU:0':\n",
        "    print('GPU device not found!!! Using CPU')\n",
        "    # raise SystemError('GPU device not found')\n",
        "    global CPU\n",
        "    CPU = True\n",
        "  print('Found GPU at: {}'.format(device_name))"
      ],
      "id": "4203506e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ioqp7yRmEEpU"
      },
      "source": [
        "import gym\n",
        "from gym.wrappers import Monitor\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "from google.colab import files,drive\n",
        "\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "\"\"\"\n",
        "Utility functions to enable video recording of gym environment \n",
        "and displaying it.\n",
        "To enable video, just do \"env = wrap_env(env)\"\"\n",
        "\"\"\"\n",
        "\n",
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    print(video)\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ],
      "id": "Ioqp7yRmEEpU",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cnPPyY_EG6x"
      },
      "source": [
        "# env = wrap_env(gym.make(\"MountainCar-v0\"))\n",
        "with tf.device('/device:GPU:0'):   \n",
        "\n",
        "  for i in range(3):\n",
        "    env = wrap_env(gym.make(\"MsPacman-v0\")) \n",
        "    observation = env.reset()\n",
        "    while True:\n",
        "          \n",
        "          # env.render()\n",
        "          #your agent goes here\n",
        "          action = env.action_space.sample() \n",
        "          if(i==1): action = env.action_space.sample()\n",
        "\n",
        "          observation, reward, done, info = env.step(action)    \n",
        "              \n",
        "          if done:\n",
        "              env.close()\n",
        "              break;\n",
        "\n",
        "show_video()"
      ],
      "id": "6cnPPyY_EG6x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTD9EeUYrfBt"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "id": "iTD9EeUYrfBt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51356073"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "import scipy.misc\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "from skimage import transform\n",
        "from google.colab import files,drive\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.math import add,reduce_mean\n",
        "from tensorflow.keras.layers import Input, Add, Dense, NoisyDense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, \\\n",
        "                                    AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "from tensorflow.keras.models import Model, load_model, Sequential\n",
        "from tensorflow.keras.losses import MeanSquaredError \n",
        "from tensorflow.keras.metrics import MeanSquaredError\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.initializers import random_uniform, glorot_uniform, constant, identity\n"
      ],
      "id": "51356073",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7a4203a0"
      },
      "source": [
        "env = wrap_env(gym.make('MsPacman-v0'))\n",
        "input_shape = (1,84,84,4)\n",
        "classes = env.action_space.n\n",
        "env.close()\n",
        "print(input_shape, classes)"
      ],
      "id": "7a4203a0",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcuzEMbY3kIf"
      },
      "source": [
        "# For saving results in drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "WcuzEMbY3kIf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0df84790"
      },
      "source": [
        "def identity_block(X, f, filters, training=True, initializer=random_uniform):\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value. You'll need this later to add back to the main path. \n",
        "    X_shortcut = X\n",
        "    \n",
        "    # First component of main path\n",
        "    X = Conv2D(filters = F1, kernel_size = 1, strides = (1,1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training = training) # Default axis\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    ## Second component of main path (≈3 lines)\n",
        "    X = Conv2D(filters = F2, kernel_size = f, strides = (1,1), padding = 'same', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training = training) # Default axis\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    ## Third component of main path (≈2 lines)\n",
        "    X = Conv2D(filters = F3, kernel_size = 1, strides = (1,1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training = training) # Default axis\n",
        "    \n",
        "    ## Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n",
        "    X = Add()([X_shortcut,X])\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    return X"
      ],
      "id": "0df84790",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cbd5f7b"
      },
      "source": [
        "def convolutional_block(X, f, filters, s = 2, training=True, initializer=glorot_uniform):\n",
        "\n",
        "    # Retrieve Filters\n",
        "    F1, F2, F3 = filters\n",
        "    \n",
        "    # Save the input value\n",
        "    X_shortcut = X\n",
        "\n",
        "    ##### MAIN PATH #####\n",
        "    \n",
        "    # First component of main path glorot_uniform(seed=0)\n",
        "    X = Conv2D(filters = F1, kernel_size = 1, strides = (s, s), padding='valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training=training)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    ## Second component of main path (≈3 lines)\n",
        "    X = Conv2D(filters = F2, kernel_size = f, strides = (1,1), padding = 'same', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training = training) # Default axis\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    ## Third component of main path (≈2 lines)\n",
        "    X = Conv2D(filters = F3, kernel_size = 1, strides = (1,1), padding = 'valid', kernel_initializer = initializer(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X, training = training) # Default axis\n",
        "    \n",
        "    ##### SHORTCUT PATH ##### (≈2 lines)\n",
        "    X_shortcut = Conv2D(filters = F3, kernel_size = 1, strides = (s,s), padding = 'valid', kernel_initializer = initializer(seed=0))(X_shortcut)\n",
        "    X_shortcut = BatchNormalization(axis = 3)(X_shortcut, training = training) # Default axis\n",
        "    \n",
        "    # Final step: \n",
        "    X = Add()([X, X_shortcut])\n",
        "    X = Activation('relu')(X)\n",
        "    \n",
        "    return X"
      ],
      "id": "9cbd5f7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7fec656"
      },
      "source": [
        "def ResNet50(input_shape, classes): # input and classes depends on gym environment\n",
        "    \"\"\"\n",
        "    Stage-wise implementation of the architecture of the popular ResNet50:\n",
        "    CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> CONVBLOCK -> IDBLOCK*2 -> CONVBLOCK -> IDBLOCK*3\n",
        "    -> CONVBLOCK -> IDBLOCK*5 -> CONVBLOCK -> IDBLOCK*2 -> AVGPOOL -> FLATTEN -> DENSE \n",
        "\n",
        "    Arguments:\n",
        "    input_shape -- shape of the images of the dataset\n",
        "    classes -- integer, number of classes\n",
        "\n",
        "    Returns:\n",
        "    model -- a Model() instance in Keras\n",
        "    \"\"\"\n",
        "    \n",
        "    # Define the input as a tensor with shape input_shape\n",
        "    X_input = Input(input_shape)\n",
        "    \n",
        "    # Zero-Padding\n",
        "    X = ZeroPadding2D((3, 3))(X_input)\n",
        "    \n",
        "    # Stage 1\n",
        "    X = Conv2D(64, (7, 7), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = BatchNormalization(axis = 3)(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
        "\n",
        "    # Stage 2\n",
        "    X = convolutional_block(X, f = 3, filters = [64, 64, 256], s = 1)\n",
        "    X = identity_block(X, 3, [64, 64, 256])\n",
        "    X = identity_block(X, 3, [64, 64, 256])\n",
        "    \n",
        "    ## Stage 3 (≈4 lines)\n",
        "    X = convolutional_block(X, f = 3, filters = [128, 128, 512], s = 2)\n",
        "    X = identity_block(X, 3, [128, 128, 512])\n",
        "    X = identity_block(X, 3, [128, 128, 512])\n",
        "    X = identity_block(X, 3, [128, 128, 512]) \n",
        "    \n",
        "    ## Stage 4 (≈6 lines)\n",
        "    X = convolutional_block(X, f = 3, filters = [256, 256, 1024], s = 2)\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "    X = identity_block(X, 3, [256, 256, 1024])\n",
        "\n",
        "    ## Stage 5 (≈3 lines)\n",
        "    X = convolutional_block(X, f = 3, filters = [512, 512, 2048], s = 2)\n",
        "    X = identity_block(X, 3, [512, 512, 2048])\n",
        "    X = identity_block(X, 3, [512, 512, 2048])\n",
        "\n",
        "    ## AVGPOOL (≈1 line). Use \"X = AveragePooling2D(...)(X)\"\n",
        "    X = AveragePooling2D(pool_size = (2,2))(X)\n",
        "\n",
        "    # output layer\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(classes, activation='softmax', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    \n",
        "    # Create model\n",
        "    model = Model(inputs = X_input, outputs = X)\n",
        "\n",
        "    return model"
      ],
      "id": "a7fec656",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b379db3a"
      },
      "source": [
        "def Deepminds_model(input_shape, classes):\n",
        "        \n",
        "        \n",
        "    X_input = Input(input_shape)\n",
        "    \n",
        "    X = Conv2D(32, (8, 8), strides = (4, 4), kernel_initializer = glorot_uniform(seed=0))(X_input)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2D(64, (4, 4), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2D(64, (3, 3), strides = (1, 1), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Flatten()(X)\n",
        "    X = Dense(512, activation='relu', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = Dense(classes, activation= None, kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    \n",
        "    model = Model(inputs = X_input, outputs = X)\n",
        "\n",
        "    return model\n",
        "    \n",
        "    "
      ],
      "id": "b379db3a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgRyLhM8Mrz7"
      },
      "source": [
        "def ConvDuelingDQN(input_shape, classes):\n",
        "\n",
        "    \"\"\"\n",
        "    We split the network into two separate streams, one for estimating the state-value and\n",
        "    the other for estimating state-dependent action advantages.\n",
        "\n",
        "    The last module of the neural network implements forward mapping shown below: \n",
        "\n",
        "    Q(s,a;theta,alpha.beta) = V(s;theta,beta)  + A(s,a;theta,alpha) - sigma A(s,a;theta,alpha)/|A|\n",
        "\n",
        "    # Here argmax_a A can also be used instead of mean A\n",
        "    \"\"\"\n",
        "\n",
        "    X_input = Input(input_shape)\n",
        "    \n",
        "    X = Conv2D(32, (8, 8), strides = (4, 4), kernel_initializer = glorot_uniform(seed=0))(X_input)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2D(64, (4, 4), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2D(64, (3, 3), strides = (1, 1), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Flatten()(X)\n",
        "\n",
        "    # Value stream\n",
        "    V = Dense(128,activation='relu', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    V = Dense(1,activation=None, kernel_initializer = glorot_uniform(seed=0))(V)\n",
        "\n",
        "    # Action stream\n",
        "    A = Dense(128,activation='relu',kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    A = Dense(classes, activation=None, kernel_initializer= glorot_uniform(seed=0))(A)\n",
        "\n",
        "    Q = add(V, add(A, -reduce_mean(A)/ classes ))\n",
        "\n",
        "    model = Model(inputs = X_input, outputs = Q)\n",
        "    \n",
        "    return model\n"
      ],
      "id": "BgRyLhM8Mrz7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFlyZ2ejRdA4"
      },
      "source": [
        "def NoisyNet_Dueling(input_shape, classes):\n",
        "    \"\"\"\n",
        "    Input :Env Environment; ε set of random variables of the network\n",
        "    Input :DUELING Boolean; \"true\" for NoisyNet-Dueling and \"false\" for NoisyNet-DQN\n",
        "    Input :B empty replay buffer; ζ initial network parameters; ζ − initial target network parameters\n",
        "    Input :N B replay buffer size; N T training batch size; N − target network replacement frequency\n",
        "    Output :Q(·, ε; ζ) action-value function\n",
        "\n",
        "    for episode e ∈ {1, . . . , M } do\n",
        "      Initialise state sequence x 0 ∼ Env\n",
        "        for t ∈ {1, . . . } do\n",
        "              /* l[−1] is the last element of the list l */\n",
        "              Set x ← x 0\n",
        "              Sample a noisy network ξ ∼ ε\n",
        "              Select an action a ← argmax b∈A Q(x, b, ξ; ζ)\n",
        "              Sample next state y ∼ P (·|x, a), receive reward r ← R(x, a) and set x 0 ← y\n",
        "              Add transition (x, a, r, y) to the replay buffer B[−1] ← (x, a, r, y)\n",
        "              if |B| > N B then\n",
        "                  Delete oldest transition from B\n",
        "              end\n",
        "              /* D is a distribution over the replay, it can be uniform or\n",
        "              implementing prioritised replay  */\n",
        "              Sample a minibatch of N T transitions ((x j , a j , r j , y j ) ∼ D)_{j=1}^{N T}\n",
        "              /* Construction of the target values.  */\n",
        "              Sample the noisy variable for the online network ξ ∼ ε\n",
        "              Sample the noisy variables for the target network ξ 0 ∼ ε\n",
        "              if DUELING then\n",
        "                  Sample the noisy variables for the action selection network ξ 00 ∼ ε\n",
        "              for j ∈ {1, . . . , N T } do\n",
        "                  if y j is a terminal state then\n",
        "                    Q' ← r j\n",
        "                  if DUELING then\n",
        "                    b ∗ (y j ) = arg max b∈A Q(y j , b, ξ 00 ; ζ)\n",
        "                    Q' ← r j + γQ(y j , b ∗ (y j ), ξ 0 ; ζ − )\n",
        "                  else\n",
        "                    Q' ← r j + γ max b∈A Q(y j , b, ξ 0 ; ζ − )\n",
        "                    Q' − Q(x j , a j , ξ; ζ)) 2\n",
        "                    Do a gradient step with loss ( Q\n",
        "              end\n",
        "                  if t ≡ 0 (mod N − ) then\n",
        "              Update the target network: ζ − ← ζ\n",
        "        end\n",
        "    end\n",
        "\n",
        "Instead of choosing a noise sample before feeding the states into the model,\n",
        "we choose the noise samle in the model itself such that whenever a batch or a single state is \n",
        "passed a different set of gaussian noise sample is obtained.\n",
        "\n",
        "There is a NoisyDense layer in tf.model.layers :)\n",
        "    \"\"\"\n",
        "    X_input = Input(input_shape)\n",
        "    \n",
        "    X = Conv2D(32, (8, 8), strides = (4, 4), kernel_initializer = glorot_uniform(seed=0))(X_input)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2D(64, (4, 4), strides = (2, 2), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Conv2D(64, (3, 3), strides = (1, 1), kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    X = Activation('relu')(X)\n",
        "    X = Flatten()(X)\n",
        "\n",
        "    # Value stream\n",
        "    V = Dense(128,activation='relu', kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    V = NoisyDense(1, activation=None, use_factorised = True, kernel_initializer = glorot_uniform(seed=0))(V)\n",
        "\n",
        "    # Action stream\n",
        "    A = Dense(128,activation='relu',kernel_initializer = glorot_uniform(seed=0))(X)\n",
        "    A = NoisyDense(classes, activation=None, use_factorised = True, kernel_initializer= glorot_uniform(seed=0))(A)\n",
        "\n",
        "    Q = add(V, add(A, -reduce_mean(A)/ classes ))\n",
        "\n",
        "    model = Model(inputs = X_input, outputs = Q)\n",
        "\n",
        "    return model\n"
      ],
      "id": "AFlyZ2ejRdA4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54dd6a46"
      },
      "source": [
        "def scale_lumininance(img):\n",
        "    return np.dot(img[...,:3],[0.299,0.587,0.114])"
      ],
      "id": "54dd6a46",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b86ff3b"
      },
      "source": [
        "def preprocess_observation(obs):\n",
        "    \"\"\"\n",
        "    To reduce the computation load we preprocess the date obtained from the atari-game env by\n",
        "    reducing the resolution and adjusting the color.\n",
        "    \n",
        "    DeepMind took the maximum pixel value over subsequent frames to reduce flickering\n",
        "    caused by the limitations of the Atari platform and then scale it from its current \n",
        "    210×160×3 resolution to 84×84.\n",
        "    \n",
        "    To convert this, we will take the luminance channel (denoted as Y) from the image, which is the our RGB channel, and apply linear weights to \n",
        "    the channel to transform it according to the relative luminance.\n",
        "    Y = 0.299R + 0.587G + 0.114B\n",
        "    \"\"\"\n",
        "    \n",
        "    obs_gray = scale_lumininance(obs)\n",
        "    obs_trans = transform.resize(obs_gray,(84,84))\n",
        "    return np.moveaxis(obs_trans, 1, 0)"
      ],
      "id": "6b86ff3b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL9CbWoK5cTM"
      },
      "source": [
        "class Prioritized_replay_buffer():\n",
        "    def __inti__(self,buffer_size):\n",
        "      \"\"\"\n",
        "      Before application of Prioritized experience replay samples were choosen randomly using random.sample\n",
        "      But now every sample were choosen according the their priority and the weights of the network were also updated \n",
        "      in accordance to these priorities.\n",
        "\n",
        "      There can be two types of prioritization.\n",
        "      Here we are using proportional based with hyperparameter a = 0.7, b = 0.5 \n",
        "\n",
        "      # sampling                              # parameter updates\n",
        "      error = Q(s,a) - Q_target               loss = error^2\n",
        "      p = |error| + offset                    theta -> theta - alpha * w * grad\n",
        "      p_r(i) = p_i^a/(sigma p^a)              w_i = (1/N * 1/Pr_(i))^b  # importance_weight\n",
        "      \n",
        "      a => priority_scale (0.7)\n",
        "      # remember to add this update at the time of eps-decay\n",
        "      b => 0.6 -- 1       # fully compensates for the non-uniform probabilities\n",
        "      \"\"\"      \n",
        "\n",
        "\n",
        "      self.buffer = deque(maxlen = buffer_size)\n",
        "      self.buffer_size = buffer_size\n",
        "      self.priorities = deque(maxlen = buffer_size) # a different deque\n",
        "      self.initialize(initial_collect_step)\n",
        "      \n",
        "    def initialize(self,initial_collect_step):\n",
        "\n",
        "        observation = env.reset()\n",
        "        current_state = pre_process(observation) # 84,84 grascale frame\n",
        "        current_state = np.stack((current_state, current_state, current_state, current_state), axis=2) # for initialization the first frame is repeated four times\n",
        "        current_state = np.reshape([current_state], (1, 84, 84, 4))        \n",
        "        \n",
        "        for i in range(initial_collect_steps):\n",
        "        \n",
        "            current_state = preprocess_observation(observation)\n",
        "            action = env.action_space.sample()\n",
        "            observation, reward, done , _ = env.step(action)\n",
        "            next_state = pre_process(observation) # 84,84 grascale frame \n",
        "            next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
        "            next_state = np.append(next_state, current_state[:, :, :, :3], axis=3)\n",
        "\n",
        "            if done: \n",
        "              observation = env.reset()\n",
        "        \n",
        "            experience = (current_state, action, reward, next_state, done)\n",
        "            self.add(experience)\n",
        "\n",
        "            current_state = next_state\n",
        "        \n",
        "        env.close()\n",
        "    \n",
        "    def add(self,experience):\n",
        "      self.buffer.append(experience)\n",
        "      self.priorities.append(max(self.priorities, default = 1))\n",
        "\n",
        "    def get_probabilities(self, priority_scale):\n",
        "      scaled_priorities = np.array(self.priorities) ** priority_scale\n",
        "      sample_probabilities = scaled_priorities/ sum(scaled_priorities)\n",
        "\n",
        "    def get_importance(self, probabilities):\n",
        "      importance = 1/self.buffer_size * 1/probabilities\n",
        "      # this has been normalized to keep the update_step size bounded\n",
        "      importance_normalized = importance/max(importance)\n",
        "      return importance_normalized\n",
        "\n",
        "    def sample(self, batch_size, priority_scale = 1.0):\n",
        "      sample_size = min(len(self.buffer), batch_size)\n",
        "      sample_probs = self.get_probabilities(priority_scale)\n",
        "      sample_indices = random.choices(range(self.buffer_size), k=sample_size, weights=sample_probs)\n",
        "      samples = np.array(self.buffer)[sample_indices]\n",
        "      importance = self.get_importance(sample_probs[sample_indices])\n",
        "      return samples, importance , sample_indices\n",
        "\n",
        "    def set_priorities(self, indices, errors, offset=0.1):\n",
        "        for i,e in zip(indices, errors):\n",
        "            self.priorities[i] = abs(e) + offset\n",
        "\n",
        "    def save(self,save_index):\n",
        "        buffer_dir = '/content/drive/MyDrive/pacman_SOC_outputs/buffers'\n",
        "        if not os.path.exists(buffer_dir):\n",
        "              raise SystemError('buffer_dir not found: make a directory at path /content/drive/MyDrive/pacman_SOC_outputs/buffers')\n",
        "        pickle.dump(self.buffer,open(os.path.join(buffer_dir,'replay-{},dat'.format(save_index),'wb')))\n"
      ],
      "id": "bL9CbWoK5cTM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1b15339"
      },
      "source": [
        "def initialization(input_shape , classes, initial_collect_steps, buffer_size):\n",
        "    \"\"\"\n",
        "    The target_network and online_network are initialized as Noisy_Dueling CNN architecture with trainability\n",
        "    of online_network set to False.\n",
        "    \n",
        "    The Experience Reply is initialized as a deque whose each element \n",
        "    is a tuple (current state, action, reward , done, next state)\n",
        "    Few actions are executed with the environment to bootstrap the replay data.\n",
        "    \"\"\"\n",
        "    global target_network, prediction_network \n",
        "    target_network = Noisy_Dueling(input_shape, classes)\n",
        "    print(target_network.summary())\n",
        "    \n",
        "    prediction_network = Noisy_Dueling(input_shape, classes)\n",
        "    print(prediction_network.summary())\n",
        "    \n",
        "    experience_replay = Prioritized_replay_buffer(buffer_size,initial_collect_step)"
      ],
      "id": "a1b15339",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1da5cd81"
      },
      "source": [
        "def plot(CR , episode, save_fig = True):\n",
        "    \n",
        "    \"\"\"\n",
        "    plot of cumulative reward at the end of each episode\n",
        "    plot being saved at each log_interval \n",
        "    \"\"\"\n",
        "    plt.plot([i+1 for i in range(episode)],CR, linewidth=4, label = \"NoisyNet n-step PDD DQN\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Episode_reward\")\n",
        "    leg = plt.legend(loc='upper left', shadow=True)\n",
        "    plt.grid()\n",
        "    ax = plt.gca()\n",
        "    plt.xlim([0, episode])\n",
        "    \n",
        "    plot_dir = '/content/drive/MyDrive/pacman_SOC_outputs/plots'\n",
        "    cum_rewards_dir = '/content/drive/MyDrive/pacman_SOC_outputs/cum_rewards'\n",
        "    if not os.path.exists(cum_rewards_dir):\n",
        "        raise SystemError('cum_rewards_dir not found: make a directory at path /content/drive/MyDrive/pacman_SOC_outputs/cum_rewards')\n",
        "    if not os.path.exists(plot_dir):\n",
        "        raise SystemError('plot_dir not found: make a directory at path /content/drive/MyDrive/pacman_SOC_outputs/plots')\n",
        "\n",
        "    pickle.dump(self.buffer,open(os.path.join(cum_rewards_dir,'cum_reward-{},dat'.format(episode),'wb'))) \n",
        "    if save_fig:\n",
        "        plt.savefig(os.path.join(plot_dir, \"plot-{}th-episode.png\".format(episode)), bbox_inches=\"tight\")\n",
        "    else:\n",
        "        plt.show()\n",
        "    plt.close()"
      ],
      "id": "1da5cd81",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9XyCNMrytuK",
        "outputId": "a5c3909d-7a48-4dd7-b731-9e2bf7d95198"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "h9XyCNMrytuK",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yxL6IYkOpWu"
      },
      "source": [
        "def get_n_step_info(self,n_step_buffer,gamma):\n",
        "    \"\"\"\n",
        "    This function has been made for the implementation of n_step Temporal Difference method in Prioritized Replay Buffer\n",
        "    n_step_buffer stores the (s,a,r,n_s,d) for n_step (3 here) previous states\n",
        "    S_{t-n_step+1} --> S_{t-n_step+2}\n",
        "    S_{t-n_step+2} --> S_{t-n_step+3}\n",
        "    ....\n",
        "    S_t  --> S_{t+1}\n",
        "    \n",
        "    In our n_step_buffer (a list of size n_step) we are adding temp_transition : (S_t, action_t, r_t, S_{t+1}, done_t)\n",
        "    \n",
        "    action_t => action for transition S_t -> S_{t+1}\n",
        "    r_t => reward for transition S_t -> S_{t+1}\n",
        "    done_t => done for S_{t+1} i.e, whether it is terminal or not\n",
        "\n",
        "    This function takes in the n_step_buffer and gamma and outputs the experience to be fed into the experience_replay.\n",
        "    experience:  (S_{t-n_step+1} , action_{t-n_step+1}, rho, S_{t+1}, done_t)\n",
        "\n",
        "    action_{t-n_step+1} => action for transition S_{t-n_step+1} -> S_{t-n_step+2}\n",
        "    rho => sum of discounted reward \n",
        "    done_t => done for S_{t+1} i.e, whether it is terminal or not\n",
        "\n",
        "    These values of experience ensures that we don't need to change the rest of the code for training the online network\n",
        "    \"\"\"\n",
        "\n",
        "    # info of the last transition\n",
        "    reward, next_state, done = n_step_buffer[-1][-3:]\n",
        "\n",
        "    for transition in reversed(list(n_step_buffer)[:-1]):\n",
        "        r, n_s, d = transition[-3:]\n",
        "\n",
        "        reward = r + gamma * reward * (1 - d)\n",
        "        next_state, done = (n_s, d) if d else (next_state, done)\n",
        "\n",
        "    return reward, next_state, done\n",
        "\n"
      ],
      "id": "7yxL6IYkOpWu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "771b5d7a"
      },
      "source": [
        "def experiment(N_episodes, gamma, learning_rate, target_model_change, batch_size, log_interval, priority_scale = 0.7,n_step = 3): \n",
        "  \n",
        "  # Instantiate an optimizer to train the model.\n",
        "  optimizer = tf.keras.optimizers.RMSprop(learning_rate) # used by deepminds\n",
        "  # Instantiate a loss function.\n",
        "  loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "  # Prepare the metrics.\n",
        "  train_acc_metric = tf.keras.metrics.MeanSquaredError()\n",
        "  \n",
        "  CR = []\n",
        "  with tf.device('/device:GPU:0'):\n",
        "\n",
        "    for episode in range(N_episodes):\n",
        "        start_time = time.time()\n",
        "        time_step = 0\n",
        "        env = wrap_env(gym.make('MsPacman-v0'))\n",
        "        \n",
        "        observation = env.reset()\n",
        "        current_state = pre_process(observation) # 84,84 grascale frame\n",
        "        current_state = np.stack((current_state, current_state, current_state, current_state), axis=2) # for initialization the first frame is repeated four times\n",
        "        current_state = np.reshape([current_state], (1, 84, 84, 4))          \n",
        "        \n",
        "        n_step_buffer = []\n",
        "        \n",
        "        rewards = [] # compared to total points earned in the game\n",
        "        while True:\n",
        "            \n",
        "            with tf.GradientTape() as tape:\n",
        "                \n",
        "                Q = prediction_network(np.array([current_state]), training=False)\n",
        "                action = np.random.choice(np.flatnonzero(Q.numpy() == Q.numpy().max())) \n",
        "                new_observation , reward , current_done , _ = env.step(action)\n",
        "                rewards.append(reward)\n",
        "                \n",
        "                # this part of the code appends the next_state to the set of four previous states which constitute the current_state\n",
        "                next_state = pre_process(new_observation)\n",
        "                next_state = np.reshape([next_state], (1, 84, 84, 1))\n",
        "                next_state = np.append(next_state, current_state[:, :, :, :3], axis=3) # next_state appended in the set of last four states \n",
        "                \n",
        "                temp_transition = (current_state, action, reward, next_state, current_done)\n",
        "\n",
        "                # n_step replay buffer\n",
        "                ####################################################################################################              \n",
        "                n_step_buffer.append(temp_transition)\n",
        "                if len(n_step_buffer) == n_step:  # fill the n-step buffer for the first translation\n",
        "                    # add a multi step transition\n",
        "                    rho , state , done = get_n_step_info(n_step_buffer, gamma)\n",
        "                    n_prev_state, action = self.n_step_buffer[0][:2]\n",
        "                ####################################################################################################\n",
        "\n",
        "                \n",
        "                # adding this time step experience into experience_replay               \n",
        "                experience = (n_prev_state, action, rho, state, done)\n",
        "                experience_replay.add(experience)\n",
        "                \n",
        "                \n",
        "                # sampling batch out of experience replay, forward prop to find Q_target and Q_prediction\n",
        "                ####################################################################################################\n",
        "                minibatch, importance , indices = experience_replay.sample(batch_size, priority_scale)\n",
        "                n_prev_state_minibatch = np.array([n_prev_state for (n_prev_state, action, rho , state, done) in minibatch])\n",
        "                actions = np.array([action for (n_prev_state, action, rho , state, done) in minibatch])\n",
        "                state_minibatch = np.array([state for (n_prev_state, action, rho , state, done) in minibatch])            \n",
        "\n",
        "                Q_target = target_network(state_minibatch,training = False)\n",
        "\n",
        "                target = []\n",
        "                for i, (n_prev_state, action, rho , state, done) in enumerate(minibatch):\n",
        "                    if done:\n",
        "                        target.append(rho)\n",
        "                    else:                      \n",
        "                        target.append(rho + (gamma**n_step) * Q_target.numpy()[i].max())\n",
        "\n",
        "                prediction_network_output = prediction_network(n_prev_state_minibatch, training = True)\n",
        "                prediction = [Q[action] for Q,action in zip(prediction_network_output,actions)]\n",
        "                ####################################################################################################\n",
        "                \n",
        "                \n",
        "                # error calculation, update of priority of this experience based on error, loss and backprop\n",
        "                ####################################################################################################\n",
        "                errors = abs(target - prediction)\n",
        "                experience_replay.set_priorities(indices, errors)\n",
        "                loss_value = loss_fn(target , prediction)\n",
        "                grads = tape.gradient(loss_value, prediction_network.trainable_variables)\n",
        "                optimizer.apply_gradients(zip(grads, prediction_network.trainable_variables))\n",
        "                ####################################################################################################\n",
        "\n",
        "                # Update training metric.\n",
        "                train_acc_metric.update_state(target, prediction)\n",
        "\n",
        "                # S -> S'\n",
        "                current_state = next_state\n",
        "                time_step+=1\n",
        "                \n",
        "                if time_step%100 == 0 :\n",
        "                  print(\"Time_step:{}\".format(time_step))\n",
        "                \n",
        "                if episode%target_model_change ==0: \n",
        "                    show_video()\n",
        "                    target_network.set_weights(prediction_network.get_weights()) \n",
        "              \n",
        "                if current_done:\n",
        "                \n",
        "                    # Display metrics at the end of each episode.\n",
        "                    train_acc = train_acc_metric.result()\n",
        "\n",
        "                    # Reset training metrics at the end of each episode\n",
        "                    train_acc_metric.reset_states()\n",
        "                    \n",
        "                    CR.append(np.squeeze(np.sum(np.array(rewards))))\n",
        "\n",
        "                    env.close()\n",
        "                    if (episode%log_interval==0):\n",
        "                      models_dir = '/content/drive/MyDrive/pacman_SOC_outputs/models'\n",
        "                      if not os.path.exists(output_dir): \n",
        "                          raise SystemError('model_dir not found: make a directory at path /content/drive/MyDrive/pacman_SOC_outputs/models')\n",
        "                      prediction_network.save(os.path.join(models_dir,\"model-{}.h5\".format(episode/log_interval)))\n",
        "                      experience_replay.save(episode/log_interval)\n",
        "                      plot(CR,episode+1)\n",
        "                    print(\"Episode {} finished after {} timesteps\".format(episode+1, time_step+1))\n",
        "                    print(\"{}s taken to finish this episode\".format(time.time() - start_time))\n",
        "                    break"
      ],
      "id": "771b5d7a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fe8fd3b3"
      },
      "source": [
        "learning_rate = 2.5e-3\n",
        "\n",
        "log_interval = 20\n",
        "\n",
        "initial_collect_steps = 750\n",
        "replay_buffer_max_length = 50000\n",
        "\n",
        "gamma = 0.99\n",
        "n_TD = 3 \n",
        "target_model_change = 100\n",
        "batch_size = 32\n",
        "N_episodes = 12000\n",
        "priority_scale = 0.7\n",
        "\n",
        "initialization(input_shape,classes,initial_collect_steps,replay_buffer_max_length)\n",
        "experiment(N_episodes, gamma, learning_rate, target_model_change, batch_size,eps,log_interval,priority_scale,n_TD)\n"
      ],
      "id": "fe8fd3b3",
      "execution_count": null,
      "outputs": []
    }
  ]
}