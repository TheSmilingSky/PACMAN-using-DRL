{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in c:\\users\\aksaht\\anaconda3\\lib\\site-packages (0.18.3)\n",
      "Requirement already satisfied: Pillow<=8.2.0 in c:\\users\\aksaht\\anaconda3\\lib\\site-packages (from gym) (8.0.1)\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in c:\\users\\aksaht\\anaconda3\\lib\\site-packages (from gym) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in c:\\users\\aksaht\\anaconda3\\lib\\site-packages (from gym) (1.19.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\aksaht\\anaconda3\\lib\\site-packages (from gym) (1.5.2)\n",
      "Requirement already satisfied: pyglet<=1.5.15,>=1.4.0 in c:\\users\\aksaht\\anaconda3\\lib\\site-packages (from gym) (1.5.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import gym # for environment\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam # adaptive momentum \n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04287487, -0.01476438,  0.02646055,  0.02658757])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import gym\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 4\n",
      "Number of actions: 2\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.render()\n",
    "print('Number of states: {}'.format(env.observation_space.shape[0]))\n",
    "print('Number of actions: {}'.format(env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build a simple nueral net with 2 hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class agent():\n",
    "\n",
    "        def __init__(self, env):\n",
    "            self.state_size = env.observation_space.shape[0]\n",
    "            self.action_size = env.action_space.n\n",
    "            self.gamma = 0.95\n",
    "            self.memory = deque(maxlen = 2000) \n",
    "            self.epsilon = 1 # initial exploration rate\n",
    "            self.epsilon_decay = 0.995\n",
    "            self.epsilon_min = 0.01\n",
    "            self.model = self.build_model()  \n",
    "\n",
    "        def build_model(self):\n",
    "            model = Sequential()\n",
    "            model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "            model.add(Dense(24, activation='relu'))\n",
    "            model.add(Dense(self.action_size, activation='linear'))\n",
    "            model.compile(loss='mse',optimizer=Adam(lr=0.01))\n",
    "            return model\n",
    "        \n",
    "        def remember(self, state, action, reward, next_state, done):\n",
    "            self.memory.append((state, action, reward, next_state, done))\n",
    "            \n",
    "        def act(self, state):\n",
    "            if random.uniform(0,1) <= self.epsilon:\n",
    "                return env.action_space.sample()\n",
    "            else:\n",
    "                act_values = self.model.predict(state)\n",
    "                return np.argmax(act_values[0])\n",
    "            \n",
    "        def replay(self, batch_size):        \n",
    "            if len(self.memory) < batch_size:\n",
    "                return \n",
    "            minibatch = random.sample(self.memory, batch_size) \n",
    "            for state, action, reward, next_state, done in minibatch:\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    target = reward + self.gamma * np.amax(self.model.predict(next_state)[0]) \n",
    "            train_target = self.model.predict(state) \n",
    "            train_target[0][action] = target\n",
    "            self.model.fit(state, train_target, verbose = 0) \n",
    "        def adaptiveEGreedy(self):\n",
    "            if self.epsilon > self.epsilon_min:\n",
    "                self.epsilon *= self.epsilon_decay\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQNagent=agent(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_9 (Dense)              (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 50        \n",
      "=================================================================\n",
      "Total params: 770\n",
      "Trainable params: 770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "DQNagent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0, time : 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Viewer.__del__ at 0x0000022C6DFD5280>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\AKSAHT\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 165, in __del__\n",
      "    self.close()\n",
      "  File \"C:\\Users\\AKSAHT\\anaconda3\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\", line 83, in close\n",
      "    self.window.close()\n",
      "  File \"C:\\Users\\AKSAHT\\anaconda3\\lib\\site-packages\\pyglet\\window\\win32\\__init__.py\", line 319, in close\n",
      "    super(Win32Window, self).close()\n",
      "  File \"C:\\Users\\AKSAHT\\anaconda3\\lib\\site-packages\\pyglet\\window\\__init__.py\", line 838, in close\n",
      "    app.windows.remove(self)\n",
      "  File \"C:\\Users\\AKSAHT\\anaconda3\\lib\\_weakrefset.py\", line 109, in remove\n",
      "    self.data.remove(ref(item))\n",
      "KeyError: <weakref at 0x0000022C028CE220; to 'Win32Window' at 0x0000022C74D9A940>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1, time : 12\n",
      "episode: 2, time : 28\n",
      "episode: 3, time : 36\n",
      "episode: 4, time : 14\n",
      "episode: 5, time : 17\n",
      "episode: 6, time : 16\n",
      "episode: 7, time : 19\n",
      "episode: 8, time : 12\n",
      "episode: 9, time : 15\n",
      "episode: 10, time : 12\n",
      "episode: 11, time : 15\n",
      "episode: 12, time : 10\n",
      "episode: 13, time : 10\n",
      "episode: 14, time : 16\n",
      "episode: 15, time : 15\n",
      "episode: 16, time : 9\n",
      "episode: 17, time : 11\n",
      "episode: 18, time : 9\n",
      "episode: 19, time : 10\n",
      "episode: 20, time : 10\n",
      "episode: 21, time : 11\n",
      "episode: 22, time : 13\n",
      "episode: 23, time : 9\n",
      "episode: 24, time : 10\n",
      "episode: 25, time : 9\n",
      "episode: 26, time : 10\n",
      "episode: 27, time : 9\n",
      "episode: 28, time : 11\n",
      "episode: 29, time : 9\n",
      "episode: 30, time : 9\n",
      "episode: 31, time : 10\n",
      "episode: 32, time : 9\n",
      "episode: 33, time : 9\n",
      "episode: 34, time : 9\n",
      "episode: 35, time : 10\n",
      "episode: 36, time : 10\n",
      "episode: 37, time : 9\n",
      "episode: 38, time : 9\n",
      "episode: 39, time : 13\n",
      "episode: 40, time : 9\n",
      "episode: 41, time : 9\n",
      "episode: 42, time : 9\n",
      "episode: 43, time : 10\n",
      "episode: 44, time : 9\n",
      "episode: 45, time : 9\n",
      "episode: 46, time : 10\n",
      "episode: 47, time : 9\n",
      "episode: 48, time : 9\n",
      "episode: 49, time : 10\n",
      "episode: 50, time : 10\n",
      "episode: 51, time : 11\n",
      "episode: 52, time : 10\n",
      "episode: 53, time : 9\n",
      "episode: 54, time : 9\n",
      "episode: 55, time : 9\n",
      "episode: 56, time : 10\n",
      "episode: 57, time : 8\n",
      "episode: 58, time : 9\n",
      "episode: 59, time : 11\n",
      "episode: 60, time : 11\n",
      "episode: 61, time : 9\n",
      "episode: 62, time : 13\n",
      "episode: 63, time : 9\n",
      "episode: 64, time : 9\n",
      "episode: 65, time : 17\n",
      "episode: 66, time : 68\n",
      "episode: 67, time : 24\n",
      "episode: 68, time : 41\n",
      "episode: 69, time : 30\n",
      "episode: 70, time : 76\n",
      "episode: 71, time : 43\n",
      "episode: 72, time : 26\n",
      "episode: 73, time : 19\n",
      "episode: 74, time : 13\n",
      "episode: 75, time : 16\n",
      "episode: 76, time : 18\n",
      "episode: 77, time : 20\n",
      "episode: 78, time : 19\n",
      "episode: 79, time : 15\n",
      "episode: 80, time : 17\n",
      "episode: 81, time : 13\n",
      "episode: 82, time : 17\n",
      "episode: 83, time : 14\n",
      "episode: 84, time : 17\n",
      "episode: 85, time : 16\n",
      "episode: 86, time : 16\n",
      "episode: 87, time : 12\n",
      "episode: 88, time : 16\n",
      "episode: 89, time : 16\n",
      "episode: 90, time : 16\n",
      "episode: 91, time : 13\n",
      "episode: 92, time : 15\n",
      "episode: 93, time : 15\n",
      "episode: 94, time : 16\n",
      "episode: 95, time : 18\n",
      "episode: 96, time : 14\n",
      "episode: 97, time : 12\n",
      "episode: 98, time : 10\n",
      "episode: 99, time : 12\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "episodes = 100\n",
    "for e in range(episodes):\n",
    "        \n",
    "        # initialize environment\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1,4])\n",
    "        \n",
    "    time = 0 # each second I will get reward, because I want to sustain a balance forever\n",
    "    while True:\n",
    "            \n",
    "            # act\n",
    "        action = DQNagent.act(state)\n",
    "            \n",
    "            # step\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1,4])\n",
    "            \n",
    "            # remember / storage\n",
    "        DQNagent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            # update state\n",
    "        state = next_state\n",
    "            \n",
    "            # replay\n",
    "        DQNagent.replay(batch_size)\n",
    "            \n",
    "            # adjust epsilon\n",
    "        DQNagent.adaptiveEGreedy()\n",
    "            \n",
    "        time += 1\n",
    "            \n",
    "        if done:\n",
    "            print('episode: {}, time : {}'.format(e, time))\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
